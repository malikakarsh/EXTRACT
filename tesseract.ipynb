{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F5o5Dr7o2C5J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import PIL\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image,ImageTk\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import pytesseract\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr32MeZO2C5P",
    "outputId": "dff223f6-054c-487c-9fe0-3deb9dd37e78"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Flatten\n",
    "from keras.models import Sequential, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5IPOJWj2C5Q",
    "outputId": "7a5a5097-404f-422c-8769-5c63ab04295c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the location of the image : sentences/say.png\n"
     ]
    }
   ],
   "source": [
    "location = input(\"Enter the location of the image : \")\n",
    "image = cv2.imread(location,0)\n",
    "thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zp38pZiS2C5S"
   },
   "outputs": [],
   "source": [
    "result = cv2.GaussianBlur(thresh, (5,5), 0)\n",
    "result = 255 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FHKgkjo62C5T"
   },
   "outputs": [],
   "source": [
    "cv2.imwrite('final_img.png',result)\n",
    "loc = 'final_img.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tz5pGi3o2C5U"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('DATA/train/.DS_Store'):\n",
    "    os.remove('DATA/train/.DS_Store')\n",
    "if os.path.isfile('DATA/test/.DS_Store'):\n",
    "    os.remove('DATA/test/.DS_Store')\n",
    "    \n",
    "for i in range(1,80):\n",
    "    if os.path.isfile(f'DATA/train/{i}/.DS_Store'):\n",
    "        os.remove(f'DATA/train/{i}/.DS_Store')\n",
    "    if os.path.isfile(f'DATA/test/{i}/.DS_Store'):\n",
    "        os.remove(f'DATA/test/{i}/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Pyt90QGo2C5U"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                            zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "4VqT-tOu2C5V",
    "outputId": "96a9a8fd-2f49-4e96-cbfd-ffe9bd20c79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 650 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "trained_image = datagen.flow_from_directory('DATA/train',\n",
    "                                            target_size = (32,32),\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zw-MW1ks2C5W"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "38ldoY_n2C5X",
    "outputId": "0ba5bed9-6b96-482c-c973-df9606af5c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image = test_datagen.flow_from_directory('DATA/test',\n",
    "                                            target_size = (32,32),\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "diz2LPpK3soj"
   },
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top = False, input_shape = (32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "VOqMlRV32C5X",
    "outputId": "d5b44379-464a-4328-d118-ade5d5947f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "21/21 [==============================] - 29s 1s/step - loss: 3.3305 - accuracy: 0.0277 - val_loss: 3.3969 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "21/21 [==============================] - 26s 1s/step - loss: 3.2831 - accuracy: 0.0292 - val_loss: 3.2615 - val_accuracy: 0.0312\n",
      "Epoch 3/30\n",
      "21/21 [==============================] - 26s 1s/step - loss: 3.2044 - accuracy: 0.0492 - val_loss: 2.9365 - val_accuracy: 0.0938\n",
      "Epoch 4/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 2.9019 - accuracy: 0.0738 - val_loss: 2.7367 - val_accuracy: 0.0625\n",
      "Epoch 5/30\n",
      "21/21 [==============================] - 26s 1s/step - loss: 2.5411 - accuracy: 0.1015 - val_loss: 2.1515 - val_accuracy: 0.2500\n",
      "Epoch 6/30\n",
      "21/21 [==============================] - 26s 1s/step - loss: 2.3465 - accuracy: 0.1692 - val_loss: 2.1277 - val_accuracy: 0.0625\n",
      "Epoch 7/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.9723 - accuracy: 0.2615 - val_loss: 1.6611 - val_accuracy: 0.3750\n",
      "Epoch 8/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.6937 - accuracy: 0.3708 - val_loss: 1.6930 - val_accuracy: 0.3125\n",
      "Epoch 9/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.2745 - accuracy: 0.4831 - val_loss: 1.0462 - val_accuracy: 0.5938\n",
      "Epoch 10/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.1784 - accuracy: 0.5231 - val_loss: 1.2430 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 1.0420 - accuracy: 0.5892 - val_loss: 1.0541 - val_accuracy: 0.5312\n",
      "Epoch 12/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.9254 - accuracy: 0.6338 - val_loss: 0.6522 - val_accuracy: 0.7812\n",
      "Epoch 13/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.8618 - accuracy: 0.6415 - val_loss: 0.5650 - val_accuracy: 0.8125\n",
      "Epoch 14/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.6189 - accuracy: 0.7446 - val_loss: 0.5854 - val_accuracy: 0.8125\n",
      "Epoch 15/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.5373 - accuracy: 0.8077 - val_loss: 0.9204 - val_accuracy: 0.7812\n",
      "Epoch 16/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.8736 - accuracy: 0.7015 - val_loss: 0.5309 - val_accuracy: 0.8750\n",
      "Epoch 17/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.3940 - accuracy: 0.8446 - val_loss: 0.1631 - val_accuracy: 0.9375\n",
      "Epoch 18/30\n",
      "21/21 [==============================] - 26s 1s/step - loss: 0.3018 - accuracy: 0.9046 - val_loss: 0.3046 - val_accuracy: 0.8750\n",
      "Epoch 19/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.4021 - accuracy: 0.8277 - val_loss: 0.1330 - val_accuracy: 0.9375\n",
      "Epoch 20/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.3262 - accuracy: 0.8769 - val_loss: 0.2797 - val_accuracy: 0.9375\n",
      "Epoch 21/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.2377 - accuracy: 0.9154 - val_loss: 0.1266 - val_accuracy: 0.9375\n",
      "Epoch 22/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1651 - accuracy: 0.9508 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.6406 - accuracy: 0.8385 - val_loss: 0.6794 - val_accuracy: 0.7500\n",
      "Epoch 24/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.4597 - accuracy: 0.8615 - val_loss: 0.1864 - val_accuracy: 0.9375\n",
      "Epoch 25/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1789 - accuracy: 0.9354 - val_loss: 0.3183 - val_accuracy: 0.8438\n",
      "Epoch 26/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.3106 - accuracy: 0.9031 - val_loss: 0.1517 - val_accuracy: 0.9375\n",
      "Epoch 27/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1239 - accuracy: 0.9600 - val_loss: 0.4608 - val_accuracy: 0.9375\n",
      "Epoch 28/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1824 - accuracy: 0.9523 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1356 - accuracy: 0.9508 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "21/21 [==============================] - 27s 1s/step - loss: 0.1284 - accuracy: 0.9662 - val_loss: 0.0091 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def Train_Model():\n",
    "    global trained_image, test_image\n",
    "    model = Sequential()\n",
    "    #conv_base = VGG16(weights='imagenet', include_top = False, input_shape = (32,32,3))\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(26,activation = 'softmax'))\n",
    "    model.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(trained_image, epochs = 30, validation_data = test_image, validation_steps = 1)\n",
    "    model.save('model.h5')\n",
    "    \n",
    "Train_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pPC8sgnL2C5Y"
   },
   "outputs": [],
   "source": [
    "def Predict_Model(img):\n",
    "    global trained_image\n",
    "    new_model = load_model('model.h5')\n",
    "    img = cv2.resize(img,(32,32),3)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = img / 255\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','1','2','3','4','5','6','7','8','9','0',',',';',':','?','!','.','@','#','$','%','&','(',')','{','}','[',']']\n",
    "    \n",
    "    #prediction = new_model.predict_classes(img)\n",
    "    predict_x=new_model.predict(img) \n",
    "    \n",
    "    classes_x=np.argmax(predict_x,axis=1)\n",
    "    \n",
    "    prediction = classes_x[0]\n",
    "    \n",
    "    my_dict = dict(trained_image.class_indices)\n",
    "    \n",
    "    for key,value in my_dict.items():\n",
    "        if prediction == value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZC13RDoE2C5Z"
   },
   "outputs": [],
   "source": [
    "def Word_Extract(location):\n",
    "    \n",
    "    if os.path.isdir('WORDS'):\n",
    "        shutil.rmtree('WORDS')\n",
    "        os.mkdir('WORDS')\n",
    "    else:\n",
    "        os.mkdir('WORDS')\n",
    "    img = PIL.Image.open(location)\n",
    "    d = pytesseract.image_to_data(Image.open(location) , output_type=Output.DICT)\n",
    "    n_boxes = len(d['level'])\n",
    "    select =[]\n",
    "    j =0\n",
    "    for i in range(n_boxes):\n",
    "        if d['text'][i] != '' and d['conf'][i] != '-1':\n",
    "            select.append(d['text'][i])\n",
    "            (x, y, w, h) = (d['left'][i]-20, d['top'][i]-20, d['width'][i]+20, d['height'][i]+20)\n",
    "            #cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            crop_img = img.crop((x,y,x+w,y+h))\n",
    "            crop_img.save(f'WORDS/{j}.png')\n",
    "            j += 1\n",
    "    \n",
    "    return  select\n",
    "      \n",
    "    \n",
    "select = Word_Extract(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Iy3jTM6U2C5Z"
   },
   "outputs": [],
   "source": [
    "if os.path.isdir('LETTERS'):\n",
    "    shutil.rmtree('LETTERS')\n",
    "    os.mkdir('LETTERS')\n",
    "else:\n",
    "    os.mkdir('LETTERS')\n",
    "    \n",
    "    \n",
    "for i in range(len(os.listdir('WORDS'))):\n",
    "    os.mkdir(f'LETTERS/{i}')\n",
    "    img = Image.open(f\"WORDS/{i}.png\")\n",
    "    img = img.resize((1600 , 800))\n",
    "    w,h = img.size\n",
    "    letters = pytesseract.image_to_boxes(img , output_type=Output.DICT)\n",
    "    letters\n",
    "    \n",
    "    idx = 0 \n",
    "    for c in range(len(letters['char'])): \n",
    "        \n",
    "        (x, y, w, h) = (letters['left'][c], letters['bottom'][c], letters['right'][c], letters['top'][c])\n",
    "        \n",
    "        crop_img = img.crop((x-50,y-50 , w+50,h+50))\n",
    "        crop_img.save(f\"LETTERS/{i}/\" + str(idx) + '.png')\n",
    "        idx+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3Fx-5j4n2C5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015BBC10B160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015BBC10BDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for i in range(len(os.listdir('LETTERS'))):\n",
    "    string = ''\n",
    "    char_dict = {}\n",
    "    for j in range(len(os.listdir(f'LETTERS/{i}'))):\n",
    "        img = cv2.imread(f'LETTERS/{i}/{j}.png')\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        char = Predict_Model(img)\n",
    "        \n",
    "        \n",
    "        for l in range(len(select[i])):\n",
    "            if char == select[i][l]:\n",
    "                char_dict[select[i].index(char)] = char\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                for k in range(len(select[i])):\n",
    "                    if k != l:\n",
    "                        if char == select[i][k]:\n",
    "                            char_dict[k] = char \n",
    "        \n",
    "        keys = list(char_dict.keys())\n",
    "        if char not in select[i]:\n",
    "            for val in range(len(select[i])):\n",
    "                if val not in keys:\n",
    "                    char_dict[val] = char\n",
    "    keys = list(char_dict.keys())\n",
    "    keys.sort()\n",
    "    #print(keys)\n",
    "    \n",
    "    for m in keys:\n",
    "        string += char_dict[m]\n",
    "        \n",
    "        \n",
    "    string += ' '\n",
    "    text += string\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AjpZgjRr2C5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The predicted sentence is:  AKARSH MALIK \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"\")\n",
    "print(\"The predicted sentence is: \",text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tesseract.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
